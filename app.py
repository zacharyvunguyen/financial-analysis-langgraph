"""
Financial Performance Reporting Agent

This application analyzes financial data, researches competitors,
and generates comprehensive financial reports using LangGraph and Google Vertex AI.
"""

import os
import time
import json
from io import StringIO
from typing import TypedDict, List, Dict, Any, Tuple, Optional

# Data processing
import pandas as pd

# Environment and configuration
from dotenv import load_dotenv

# Google Cloud and API integrations
from google.oauth2 import service_account
from tavily import TavilyClient
from langchain_google_vertexai import ChatVertexAI
from langchain_community.tools.tavily_search import TavilySearchResults

# LangChain and LangGraph components
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Streamlit UI
import streamlit as st

# =============================================================================
# Configuration and Environment Setup
# =============================================================================

def load_environment_variables():
    """Load and validate environment variables from .env file."""
    load_dotenv()
    
    # Required environment variables
    required_vars = [
        "GOOGLE_CLOUD_PROJECT_ID", 
        "GOOGLE_CLOUD_REGION", 
        "GOOGLE_APPLICATION_CREDENTIALS", 
        "TAVILY_API_KEY"
    ]
    
    # Collect missing variables
    missing_vars = []
    env_values = {}
    
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            missing_vars.append(var)
        else:
            env_values[var] = value
    
    # If there are missing variables, provide a clear error message
    if missing_vars:
        error_msg = f"Missing required environment variables: {', '.join(missing_vars)}"
        print(f"ERROR: {error_msg}")
        return None, error_msg
    
    # Return configuration dictionary
    return {
        "project_id": env_values["GOOGLE_CLOUD_PROJECT_ID"],
        "region": env_values["GOOGLE_CLOUD_REGION"],
        "service_account_file": env_values["GOOGLE_APPLICATION_CREDENTIALS"],
        "tavily_key": env_values["TAVILY_API_KEY"],
        "model_name": "gemini-1.0-pro",  # Using a widely available model
        "temperature": float(os.getenv("TEMPERATURE", "0.1")),
        "max_output_tokens": int(os.getenv("MAX_OUTPUT_TOKENS", "4096"))
    }, None

def initialize_api_clients(config):
    """Initialize Google Vertex AI and Tavily API clients."""
    try:
        # Initialize credentials
        credentials = service_account.Credentials.from_service_account_file(
            config["service_account_file"],
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )
        
        # Print debug information
        print(f"Using Project ID from env: {config['project_id']}")
        print(f"Using service account from: {config['service_account_file']}")
        print(f"Service account project ID: {credentials.project_id}")
        
        # Initialize LLM
        print(f"Initializing ChatVertexAI with project={config['project_id']}, location={config['region']}, model={config['model_name']}")
        model = ChatVertexAI(
            model_name=config["model_name"],
            project=config["project_id"],
            location=config["region"],
            credentials=credentials,
            temperature=config["temperature"],
            max_output_tokens=config["max_output_tokens"],
            streaming=True
        )
        
        # Initialize Tavily
        tavily = TavilyClient(api_key=config["tavily_key"])
        
        return model, tavily, None
    
    except Exception as e:
        error_msg = f"Error initializing API clients: {str(e)}"
        print(error_msg)
        return None, None, error_msg

# =============================================================================
# Type Definitions and Data Models
# =============================================================================

class AgentState(TypedDict):
    """Definition of the state maintained by the LangGraph agent."""
    task: str
    competitors: List[str]
    csv_file: str
    financial_data: str
    analysis: str
    competitor_data: str
    comparison: str
    feedback: str
    report: str
    content: List[str]
    revision_number: int
    max_revisions: int

class Queries(BaseModel):
    """Model for search queries generated by the LLM."""
    queries: List[str]
    
    @classmethod
    def from_response(cls, response: str) -> "Queries":
        """Parse a text response into a Queries object if structured output fails."""
        try:
            # Try to extract queries from a text response
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            
            # Look for numbered items or bullet points
            queries = []
            for line in lines:
                # Remove common prefixes like numbers, dashes, asterisks
                clean_line = line
                for prefix in ["1.", "2.", "3.", "•", "-", "*", "1)", "2)", "3)"]:
                    if line.startswith(prefix):
                        clean_line = line[len(prefix):].strip()
                        break
                if clean_line and len(queries) < 3:  # Limit to 3 queries
                    queries.append(clean_line)
            
            # If we couldn't find any queries, take the first 2-3 sentences
            if not queries and len(lines) > 0:
                combined_text = " ".join(lines)
                sentences = combined_text.split(".")
                queries = [s.strip() + "." for s in sentences[:3] if s.strip()]
                
            # If still no queries, use generic ones
            if not queries:
                queries = ["financial performance metrics", "company financial analysis"]
                
            return cls(queries=queries)
            
        except Exception as e:
            print(f"Error parsing response to Queries: {str(e)}")
            return cls(queries=["financial analysis best practices", "industry benchmarks"])

# =============================================================================
# Utility Functions
# =============================================================================

def safe_structured_output(model, output_class, messages):
    """Get structured output from an LLM with fallback mechanisms."""
    try:
        # First try using the structured output capability
        result = model.with_structured_output(output_class).invoke(messages)
        if result is None:
            # If we get None, try to parse the regular text output
            regular_response = model.invoke(messages)
            if hasattr(output_class, 'from_response'):
                return output_class.from_response(regular_response.content)
            raise ValueError("Model returned None for structured output")
        return result
    except Exception as e:
        print(f"Error in structured output: {str(e)}")
        # Try regular invoke as fallback
        try:
            regular_response = model.invoke(messages)
            if hasattr(output_class, 'from_response'):
                return output_class.from_response(regular_response.content)
            raise ValueError("Cannot parse regular response")
        except Exception as fallback_error:
            print(f"Fallback also failed: {str(fallback_error)}")
            # Use the constructor if available to create a default instance
            if hasattr(output_class, 'from_response'):
                return output_class.from_response("")
            return None

def safe_tavily_search(tavily_client, query, max_results=2):
    """Perform a Tavily search with error handling."""
    if tavily_client is None:
        print(f"Warning: Tavily client is not initialized, cannot search for: {query}")
        return {"results": [{"content": f"Unable to search for '{query}' - Tavily API not available."}]}
    
    # Add retry logic for Tavily searches
    max_retries = 3
    retry_delay = 2  # seconds
    
    for attempt in range(max_retries):
        try:
            print(f"Attempting Tavily search for: '{query}' (attempt {attempt+1}/{max_retries})")
            response = tavily_client.search(query=query, max_results=max_results)
            
            # Verify the response has the expected structure
            if not isinstance(response, dict) or "results" not in response:
                print(f"Warning: Unexpected response format from Tavily for query '{query}'")
                if attempt < max_retries - 1:
                    print(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    continue
                else:
                    return {"results": [{"content": f"Search for '{query}' returned an invalid response format."}]}
                    
            # Check if we have any results
            if not response["results"]:
                print(f"Warning: No results found for query '{query}'")
                return {"results": [{"content": f"No information found for '{query}'. The search yielded no results."}]}
                
            return response
            
        except Exception as e:
            error_msg = str(e)
            print(f"Error in Tavily search for '{query}' (attempt {attempt+1}/{max_retries}): {error_msg}")
            
            # For some errors, there's no point in retrying
            if "api_key" in error_msg.lower() or "authentication" in error_msg.lower():
                return {"results": [{"content": f"Authentication error when searching for '{query}'. Please check your Tavily API key."}]}
            
            # For network errors, rate limits, etc., retry after a delay
            if attempt < max_retries - 1:
                print(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
            else:
                return {"results": [{"content": f"Error searching for '{query}' after {max_retries} attempts: {error_msg}"}]}
    
    # This should never be reached, but just in case
    return {"results": [{"content": f"Unable to complete search for '{query}' due to unexpected errors."}]}

def extract_node_and_values(step):
    """Extract node name and values from a LangGraph step output."""
    if not isinstance(step, dict):
        return "unknown", {}
        
    # Try different known structures
    if "values" in step:
        # LangGraph newer format
        node_name = list(step.get("keys", []))[0] if "keys" in step and step["keys"] else "unknown"
        return node_name, step["values"]
    
    # Direct node output format
    for node in ["gather_financials", "analyze_data", "research_competitors", 
                "compare_performance", "collect_feedback", 
                "research_critique", "write_report"]:
        if node in step:
            return node, step[node]
    
    # Unknown format - try to extract any useful information
    for key, value in step.items():
        if isinstance(value, dict) and any(k in value for k in ["financial_data", "analysis", "content", 
                                                              "comparison", "feedback", "report"]):
            return key, value
    
    return "unknown", step

# =============================================================================
# System Prompts
# =============================================================================

PROMPTS = {
    "gather_financials": """You are an expert financial analyst. Gather the financial data for the given company. Provide detailed financial data.""",
    
    "analyze_data": """You are an expert financial analyst. Analyze the provided financial data and provide detailed insights and analysis.""",
    
    "research_competitors": """You are a researcher tasked with providing information about similar companies for performance comparison. Generate a list of search queries to gather relevant information. Only generate 3 queries max.""",
    
    "compare_performance": """You are an expert financial analyst. Compare the financial performance of the given company with its competitors based on the provided data.
**MAKE SURE TO INCLUDE THE NAMES OF THE COMPETITORS IN THE COMPARISON.**""",
    
    "feedback": """You are a reviewer. Provide detailed feedback and critique for the provided financial comparison report. Include any additional information or revisions needed.""",
    
    "write_report": """You are a financial report writer. Write a comprehensive financial report based on the analysis, competitor research, comparison, and feedback provided.""",
    
    "research_critique": """You are a researcher tasked with providing information to address the provided critique. Generate a list of search queries to gather relevant information. Only generate 3 queries max."""
}

# =============================================================================
# LangGraph Node Functions
# =============================================================================

def gather_financials_node(state: AgentState):
    """Process financial data from CSV and generate initial analysis."""
    csv_file = state["csv_file"]
    df = pd.read_csv(StringIO(csv_file))
    financial_data_str = df.to_string(index=False)
    combined_content = f"{state['task']}\n\nHere is the financial data:\n\n{financial_data_str}"

    messages = [
        SystemMessage(content=PROMPTS["gather_financials"]),
        HumanMessage(content=combined_content),
    ]

    print("Calling LLM API for gather_financials...")
    start_time = time.time()
    try:
        response = model.invoke(messages)
        print(f"LLM API response received in {time.time() - start_time} seconds")
        return {"financial_data": response.content}
    except Exception as e:
        print(f"Error calling LLM API: {str(e)}")
        return {"financial_data": "Error retrieving financial data"}

def analyze_data_node(state: AgentState):
    """Analyze financial data to extract insights."""
    messages = [
        SystemMessage(content=PROMPTS["analyze_data"]),
        HumanMessage(content=state["financial_data"]),
    ]
    response = model.invoke(messages)
    return {"analysis": response.content}

def research_competitors_node(state: AgentState):
    """Research competitors using web search."""
    content = state["content"] or []
    
    # Create a progress indicator for competitor research
    if st.session_state.get("_processing_competitors", False):
        competitor_progress = st.empty()
        competitor_progress.info(f"Researching competitors... (0/{len(state['competitors'])})")
    
    for i, competitor in enumerate(state["competitors"]):
        try:
            # Update progress indicator
            if st.session_state.get("_processing_competitors", False):
                competitor_progress.info(f"Researching competitor: {competitor} ({i+1}/{len(state['competitors'])})")
            
            # Use our safer structured output function
            queries_obj = safe_structured_output(
                model,
                Queries,
                [
                    SystemMessage(content=PROMPTS["research_competitors"]),
                    HumanMessage(content=f"Generate search queries to find financial information about {competitor}."),
                ]
            )
            
            # Should always have queries now, but check just in case
            if queries_obj is None:
                print(f"Warning: safe_structured_output returned None for competitor: {competitor}")
                default_queries = [
                    f"financial performance of {competitor}", 
                    f"{competitor} revenue and growth",
                    f"{competitor} market position and financial health"
                ]
                queries_list = default_queries
            else:
                queries_list = queries_obj.queries
            
            # Process the queries
            competitor_info = []
            for q in queries_list:
                # Ensure the query mentions the competitor name
                if competitor.lower() not in q.lower():
                    q = f"{competitor} {q}"
                    
                response = safe_tavily_search(tavily, q)
                for r in response["results"]:
                    competitor_info.append(r["content"])
            
            # Add a separator with the competitor name for better readability
            if competitor_info:
                content.append(f"--- INFORMATION ABOUT {competitor.upper()} ---")
                content.extend(competitor_info)
                
        except Exception as e:
            error_msg = f"Error in research_competitors_node for competitor {competitor}: {str(e)}"
            print(error_msg)
            content.append(f"--- INFORMATION ABOUT {competitor.upper()} ---")
            content.append(f"Error researching competitor: {error_msg}")
            
            # Create default queries when everything fails
            default_queries = [
                f"financial performance of {competitor}", 
                f"{competitor} revenue and growth",
                f"{competitor} market share"
            ]
            for q in default_queries:
                response = safe_tavily_search(tavily, q)
                for r in response["results"]:
                    content.append(r["content"])
    
    # Clear the progress indicator
    if st.session_state.get("_processing_competitors", False):
        competitor_progress.empty()
    
    # Save competitor research to session state for display
    if "competitor_content" in st.session_state:
        st.session_state.competitor_content = content
                    
    return {"content": content}

def compare_performance_node(state: AgentState):
    """Compare company performance against competitors."""
    content = "\n\n".join(state["content"] or [])
    
    # Create an enhanced prompt with specific instructions to ensure accuracy
    enhanced_prompt = f"""You are an expert financial analyst. Compare the financial performance of the given company with its competitors based on the provided data.

IMPORTANT GUIDELINES:
1. ALWAYS include the names of all competitors in your comparison
2. Be factually accurate with all numbers and percentages
3. If you see conflicting data, acknowledge the discrepancy and use the most reliable information
4. Maintain consistency between metrics (e.g., if discussing revenue growth, use the same time period for all companies)
5. Clearly differentiate between absolute values (e.g., revenue in dollars) and relative metrics (e.g., percentages)
6. Be honest about both strengths and weaknesses
7. Format your output with clear headings and tables when appropriate
8. Verify that all competitors mentioned in the task are covered in your analysis

YOUR TASK:
{state['task']}

FINANCIAL ANALYSIS:
{state['analysis']}

COMPETITOR INFORMATION:
{content}"""

    user_message = HumanMessage(content=enhanced_prompt)
    
    messages = [
        SystemMessage(content=PROMPTS["compare_performance"]),
        user_message,
    ]
    
    # For revision cycles, include previous comparison and feedback to ensure improvements
    if state.get("revision_number", 1) > 1 and state.get("feedback"):
        revision_context = f"""
PREVIOUS COMPARISON REPORT:
{state.get('comparison', '')}

FEEDBACK ON PREVIOUS REPORT:
{state.get('feedback', '')}

Please address all feedback points and ensure your updated comparison is accurate and comprehensive.
"""
        messages.append(HumanMessage(content=revision_context))
    
    response = model.invoke(messages)
    
    # Perform a basic verification to ensure all competitors are mentioned
    comparison_text = response.content
    missing_competitors = []
    for competitor in state["competitors"]:
        if competitor.lower() not in comparison_text.lower():
            missing_competitors.append(competitor)
    
    # If competitors are missing, append a note requesting inclusion
    if missing_competitors:
        additional_request = f"\n\nPlease ensure your analysis includes all competitors: {', '.join(missing_competitors)}"
        messages.append(HumanMessage(content=additional_request))
        # Get updated response with all competitors
        response = model.invoke(messages)
    
    return {
        "comparison": response.content,
        "revision_number": state.get("revision_number", 1) + 1,
    }

def research_critique_node(state: AgentState):
    """Research additional information based on critique."""
    content = state["content"] or []
    try:
        # Use our safer structured output function
        queries_obj = safe_structured_output(
            model,
            Queries,
            [
                SystemMessage(content=PROMPTS["research_critique"]),
                HumanMessage(content=state["feedback"]),
            ]
        )
        
        # Should always have queries now, but check just in case
        if queries_obj is None:
            print(f"Warning: safe_structured_output returned None for critique research")
            default_queries = ["financial analysis best practices", "company comparison metrics", "industry benchmarks"]
            queries_list = default_queries
        else:
            queries_list = queries_obj.queries
        
        # Process the queries
        for q in queries_list:
            response = safe_tavily_search(tavily, q)
            for r in response["results"]:
                content.append(r["content"])
    except Exception as e:
        print(f"Error in research_critique_node: {str(e)}")
        # Create default queries when everything fails
        default_queries = ["financial analysis best practices", "company comparison metrics", "industry benchmarks"]
        for q in default_queries:
            response = safe_tavily_search(tavily, q)
            for r in response["results"]:
                content.append(r["content"])
                
    return {"content": content}

def collect_feedback_node(state: AgentState):
    """Collect feedback on the comparison report with a focus on data accuracy."""
    
    # Enhanced prompt for fact-checking and data consistency
    accuracy_prompt = f"""You are a detail-oriented financial reviewer. Your task is to review a financial comparison report and provide detailed feedback, with special attention to:

1. DATA ACCURACY: Check if financial figures match between the report and the source data
2. CONSISTENCY: Ensure metrics are consistent across competitors
3. COMPLETENESS: Verify all competitors are adequately covered
4. LOGICAL VALIDITY: Confirm conclusions follow from the data presented
5. CLARITY: Assess if the information is presented clearly and without contradiction

If you find any discrepancies in the data (like different revenue growth percentages, conflicting profit margins, etc.), explicitly point them out and suggest corrections.

Here is the original financial analysis:
{state["analysis"]}

Here is the comparison report to review:
{state["comparison"]}

Provide structured feedback with specific points for improvement.
"""

    messages = [
        SystemMessage(content=PROMPTS["feedback"]),
        HumanMessage(content=accuracy_prompt),
    ]
    
    response = model.invoke(messages)
    
    # Add a verification step to ensure the feedback addresses data accuracy
    feedback_content = response.content
    if "data" not in feedback_content.lower() and "accuracy" not in feedback_content.lower() and "discrepancy" not in feedback_content.lower():
        # If feedback doesn't mention data accuracy issues, ask for specific data verification
        follow_up = "Please specifically verify if there are any data discrepancies or accuracy issues in the financial metrics presented in the comparison."
        messages.append(HumanMessage(content=follow_up))
        response = model.invoke(messages)
    
    return {"feedback": response.content}

def write_report_node(state: AgentState):
    """Generate the final comprehensive report with verified data accuracy."""
    
    # Create a more detailed prompt focusing on accuracy
    report_prompt = f"""You are a financial report writer. Write a comprehensive financial report based on the following information:

FINANCIAL ANALYSIS:
{state['analysis']}

COMPARISON:
{state['comparison']}

FEEDBACK:
{state.get('feedback', '')}

IMPORTANT GUIDELINES:
1. Ensure all financial metrics are consistent and accurate
2. If there are conflicting data points, explain the discrepancy and use the most reliable figures
3. Present all information in a structured, clear format
4. Include tables where appropriate to summarize key metrics
5. Format financial figures consistently (with $ signs, decimal places, etc.)
6. Clearly differentiate between absolute values and percentages
7. Address any data issues mentioned in the feedback
8. Double-check that data for all competitors is consistent throughout the report

Your report should include the following sections:
- Executive Summary
- Company Overview
- Financial Performance Analysis
- Competitor Comparison
- Strengths and Weaknesses
- Recommendations
- Conclusion
"""

    messages = [
        SystemMessage(content=PROMPTS["write_report"]),
        HumanMessage(content=report_prompt),
    ]
    
    response = model.invoke(messages)
    
    # Verify that all competitors are mentioned in the report
    report_content = response.content
    missing_competitors = []
    for competitor in state["competitors"]:
        if competitor.lower() not in report_content.lower():
            missing_competitors.append(competitor)
    
    # If competitors are missing, request an updated report
    if missing_competitors:
        additional_request = f"""Please update your report to include analysis of all competitors:
{', '.join(missing_competitors)}

Ensure these competitors are integrated throughout the report, not just mentioned in passing."""
        messages.append(HumanMessage(content=additional_request))
        response = model.invoke(messages)
    
    return {"report": response.content}

def should_continue(state):
    """Determine whether to continue the refinement loop or generate the final report."""
    if state["revision_number"] > state["max_revisions"]:
        return END
    return "collect_feedback"

# =============================================================================
# LangGraph Setup
# =============================================================================

def build_graph():
    """Build and compile the LangGraph agent workflow."""
    builder = StateGraph(AgentState)
    
    # Add nodes for each step in the workflow
    builder.add_node("gather_financials", gather_financials_node)
    builder.add_node("analyze_data", analyze_data_node)
    builder.add_node("research_competitors", research_competitors_node)
    builder.add_node("compare_performance", compare_performance_node)
    builder.add_node("collect_feedback", collect_feedback_node)
    builder.add_node("research_critique", research_critique_node)
    builder.add_node("write_report", write_report_node)
    
    # Set the entry point
    builder.set_entry_point("gather_financials")
    
    # Add conditional edges for the feedback loop
    builder.add_conditional_edges(
        "compare_performance",
        should_continue,
        {END: "write_report", "collect_feedback": "collect_feedback"},
    )
    
    # Add linear edges for the main workflow
    builder.add_edge("gather_financials", "analyze_data")
    builder.add_edge("analyze_data", "research_competitors")
    builder.add_edge("research_competitors", "compare_performance")
    builder.add_edge("collect_feedback", "research_critique")
    builder.add_edge("research_critique", "compare_performance")
    builder.add_edge("write_report", END)
    
    # Compile the graph with memory checkpointer
    memory = MemorySaver()
    return builder.compile(checkpointer=memory)

# =============================================================================
# Streamlit UI Functions
# =============================================================================

def initialize_session_state():
    """Initialize Streamlit session state variables."""
    if "analysis_complete" not in st.session_state:
        st.session_state.analysis_complete = False
        st.session_state.financial_data = ""
        st.session_state.analysis = ""
        st.session_state.competitor_content = []
        st.session_state.comparison = ""
        st.session_state.feedback = ""
        st.session_state.final_report = ""

def display_tabs():
    """Create and return tabs for displaying results."""
    return st.tabs(["Financial Data", "Analysis", "Competitor Research", "Comparison", "Final Report"])

def input_form():
    """Create the input form for analysis parameters."""
    with st.sidebar:
        st.header("Analysis Parameters")
        with st.form("analysis_form"):
            task = st.text_input(
                "Task Description",
                "Analyze the financial performance of our company (MyAICo.AI) compared to competitors"
            )
            
            competitors_text = st.text_area(
                "Competitors (one per line)", 
                "Microsoft\nNvidia\nGoogle"
            )
            competitors = [comp.strip() for comp in competitors_text.split("\n") if comp.strip()]
            
            max_revisions = st.number_input("Maximum Revision Cycles", min_value=1, value=2)
            
            st.write("Upload financial data (CSV)")
            st.info("You can use the sample financials.csv file included in the repository")
            
            uploaded_file = st.file_uploader(
                "Upload company financial data (CSV)", 
                type=["csv"]
            )
            
            submit_button = st.form_submit_button("Start Analysis")
    
    return submit_button, task, competitors, max_revisions, uploaded_file

def handle_analysis(task, competitors, max_revisions, uploaded_file, tabs):
    """Process the financial analysis workflow."""
    with st.spinner("Processing..."):
        # Read the uploaded CSV file
        if hasattr(uploaded_file, 'getvalue'):
            csv_data = uploaded_file.getvalue().decode("utf-8")
        else:
            # For StringIO objects (sample data)
            csv_data = uploaded_file.read()
            if hasattr(uploaded_file, 'seek'):
                uploaded_file.seek(0)  # Reset position for potential future reads
        
        # Initialize the graph
        print("Initializing graph...")
        graph = build_graph()
        print("Graph initialized.")
        
        # Set up initial state
        initial_state = {
            "task": task,
            "competitors": competitors,
            "csv_file": csv_data,
            "max_revisions": max_revisions,
            "revision_number": 1,
            "content": [],
            "financial_data": "",
            "analysis": "",
            "comparison": "",
            "feedback": "",
            "report": ""
        }
        thread = {"configurable": {"thread_id": "1"}}
        
        # Set a flag to indicate we're processing (for progress updates)
        st.session_state["_processing_competitors"] = True
        
        # Create progress tracking
        workflow_steps = [
            "gather_financials", 
            "analyze_data", 
            "research_competitors", 
            "compare_performance", 
            "collect_feedback", 
            "research_critique", 
            "write_report"
        ]
        
        # Create progress indicators
        st.write("### Workflow Progress")
        progress_placeholders = {}
        progress_container = st.container()
        with progress_container:
            cols = st.columns(len(workflow_steps))
            for i, step in enumerate(workflow_steps):
                with cols[i]:
                    progress_placeholders[step] = st.empty()
                    progress_placeholders[step].markdown(f"⏳ **{step.replace('_', ' ').title()}**")
        
        # Clear all tabs initially
        for tab in tabs:
            tab.empty()
        
        # Create placeholders for each tab
        financial_data_placeholder = tabs[0].container()
        analysis_placeholder = tabs[1].container()
        competitor_research_container = tabs[2].container()
        comparison_container = tabs[3].container()
        final_report_placeholder = tabs[4].container()
        
        # Process the graph and stream updates
        try:
            print("Starting graph stream...")
            
            # Execute the graph and get the final state
            for step in graph.stream(initial_state, thread):
                # Extract node name and values
                node_name, values = extract_node_and_values(step)
                
                # Update progress indicators
                if node_name in progress_placeholders:
                    progress_placeholders[node_name].markdown(f"✅ **{node_name.replace('_', ' ').title()}**")
                
                # Update UI based on the node that was executed
                update_ui_for_step(
                    node_name, 
                    values, 
                    financial_data_placeholder, 
                    analysis_placeholder, 
                    competitor_research_container,
                    comparison_container, 
                    final_report_placeholder
                )
                
                # Allow Streamlit to refresh the UI
                time.sleep(0.5)
            
            print("Graph stream completed.")
        
        except Exception as e:
            st.error(f"An error occurred during processing: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

def update_ui_for_step(node_name, values, financial_data_placeholder, analysis_placeholder, 
                      competitor_research_container, comparison_container, final_report_placeholder):
    """Update the UI based on the completed step."""
    if node_name == "gather_financials" or "financial_data" in values:
        financial_data = values.get("financial_data", "")
        if financial_data:
            # Escape dollar signs for proper Markdown rendering
            financial_data = financial_data.replace("$", "\\$")
            st.session_state.financial_data = financial_data
            financial_data_placeholder.empty()
            financial_data_placeholder.markdown(financial_data)
    
    elif node_name == "analyze_data" or "analysis" in values:
        analysis = values.get("analysis", "")
        if analysis:
            # Escape dollar signs for proper Markdown rendering
            analysis = analysis.replace("$", "\\$")
            st.session_state.analysis = analysis
            analysis_placeholder.empty()
            analysis_placeholder.markdown(analysis)
    
    elif node_name == "research_competitors" or "content" in values:
        content = values.get("content", [])
        if content:
            # Save competitor research content
            st.session_state.competitor_content = content
            
            # Update the UI
            competitor_research_container.empty()
            with competitor_research_container:
                st.write("### Competitor Research")
                for i, item in enumerate(content):
                    st.write(f"**Source {i+1}:**")
                    # Escape dollar signs for proper Markdown rendering
                    if isinstance(item, str):
                        item = item.replace("$", "\\$")
                    st.write(item)
                    st.divider()
    
    elif node_name == "compare_performance" or "comparison" in values:
        comparison = values.get("comparison", "")
        if comparison:
            # Escape dollar signs for proper Markdown rendering
            comparison = comparison.replace("$", "\\$")
            st.session_state.comparison = comparison
            comparison_container.empty()
            comparison_container.markdown(comparison)
    
    elif node_name == "collect_feedback" or "feedback" in values:
        feedback = values.get("feedback", "")
        if feedback:
            # Escape dollar signs for proper Markdown rendering
            feedback = feedback.replace("$", "\\$")
            st.session_state.feedback = feedback
            comparison_container.markdown("### Feedback")
            comparison_container.markdown(feedback)
    
    elif node_name == "write_report" or "report" in values:
        final_report = values.get("report", "")
        if final_report:
            # Escape dollar signs for proper Markdown rendering
            final_report = final_report.replace("$", "\\$")
            st.session_state.final_report = final_report
            final_report_placeholder.empty()
            final_report_placeholder.markdown(final_report)
                                    
            # Mark analysis as complete
            st.session_state.analysis_complete = True

def display_completed_analysis(tabs):
    """Display the completed analysis results in tabs."""
    # Clear all tabs first
    for tab in tabs:
        tab.empty()
        
    # Financial Data tab
    tabs[0].markdown(st.session_state.financial_data)
    
    # Analysis tab
    tabs[1].markdown(st.session_state.analysis)
    
    # Competitor Research tab
    with tabs[2]:
        st.write("### Competitor Research")
        for i, content in enumerate(st.session_state.competitor_content):
            st.write(f"**Source {i+1}:**")
            # Escape dollar signs for proper Markdown rendering
            content_escaped = content.replace("$", "\\$") if isinstance(content, str) else content
            st.write(content_escaped)
            st.divider()
    
    # Comparison tab
    tabs[3].markdown(st.session_state.comparison)
    if st.session_state.feedback:
        tabs[3].markdown("### Feedback")
        tabs[3].markdown(st.session_state.feedback)
    
    # Final Report tab - ensure we're not duplicating content
    tabs[4].markdown(st.session_state.final_report)

def display_sidebar_controls():
    """Display controls in the sidebar for completed analysis."""
    st.sidebar.success("Analysis complete!")
    
    # For the download, use the original text without escaped dollar signs
    download_report = st.session_state.final_report.replace("\\$", "$") if st.session_state.final_report else ""
    
    st.sidebar.download_button(
        label="Download Report",
        data=download_report,
        file_name="financial_analysis_report.txt",
        mime="text/plain"
    )
    
    # Add a button to start a new analysis
    if st.sidebar.button("Start New Analysis"):
        reset_session_state()
        st.rerun()

def reset_session_state():
    """Reset all session state variables."""
    st.session_state.analysis_complete = False
    st.session_state.financial_data = ""
    st.session_state.analysis = ""
    st.session_state.competitor_content = []
    st.session_state.comparison = ""
    st.session_state.feedback = ""
    st.session_state.final_report = ""

# =============================================================================
# Main Application
# =============================================================================

def main():
    """Main application entry point."""
    st.title("Financial Performance Reporting Agent")
    st.write("Upload financial data and get a comprehensive analysis compared to competitors")
    
    # Check if model is initialized
    if model is None:
        st.error("Error: Model could not be initialized. Please check your Google Cloud credentials and environment variables.")
        st.info("Make sure you have set up the required environment variables in .env file and have a valid service account key.")
        
        # Display more specific error messages if available
        if config_error:
            st.error(f"Configuration Error: {config_error}")
        if error_message:
            st.error(f"API Client Error: {error_message}")
            
        # Provide guidance on setting up environment variables
        with st.expander("Environment Variables Setup Guide"):
            st.markdown("""
            ### Required Environment Variables
            
            Create a `.env` file in the project root with the following variables:
            
            ```
            GOOGLE_CLOUD_PROJECT_ID=your-project-id
            GOOGLE_CLOUD_REGION=us-central1
            GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json
            TAVILY_API_KEY=your-tavily-api-key
            TEMPERATURE=0.1
            MAX_OUTPUT_TOKENS=4096
            ```
            
            ### Obtaining API Keys
            
            1. **Google Cloud**: Create a service account with Vertex AI access
            2. **Tavily**: Sign up at [tavily.com](https://tavily.com) to get an API key
            """)
            
        return
    
    # Initialize session state
    initialize_session_state()
    
    # Create tabs for displaying results
    tabs = display_tabs()
    
    # If analysis is not complete, show the input form
    if not st.session_state.analysis_complete:
        submit_button, task, competitors, max_revisions, uploaded_file = input_form()
        
        if submit_button:
            if not competitors:
                st.error("Please enter at least one competitor.")
            elif uploaded_file is None:
                st.error("Please upload your own CSV file.")
            else:
                try:
                    # Test Tavily API before starting the analysis
                    test_query = "test query"
                    with st.spinner("Testing API connections..."):
                        test_result = safe_tavily_search(tavily, test_query)
                        if "error" in str(test_result).lower() or "api_key" in str(test_result).lower():
                            st.error(f"Tavily API test failed. Please check your API key and connection. Error: {test_result}")
                            return
                    
                    # Start the analysis
                    handle_analysis(task, competitors, max_revisions, uploaded_file, tabs)
                except Exception as e:
                    st.error(f"An error occurred: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
    else:
        # If analysis is complete, display the results
        display_completed_analysis(tabs)
    
    # Show download button and reset button if analysis is complete
    if st.session_state.analysis_complete:
        display_sidebar_controls()

# =============================================================================
# Application Initialization
# =============================================================================

if __name__ == "__main__":
    # Load configuration from environment variables
    config, config_error = load_environment_variables()
    
    # Initialize API clients if configuration is valid
    if config:
        model, tavily, error_message = initialize_api_clients(config)
    else:
        model, tavily, error_message = None, None, "Invalid configuration"
    
    # Run the application
    main()
